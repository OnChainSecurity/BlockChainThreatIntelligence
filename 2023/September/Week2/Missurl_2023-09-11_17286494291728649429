{
    "threat_intelligence": {
        "url": "https://www.mdpi.com/2079-9292/12/18/3893",
        "timestamp": "2023-09-11 00:00:00",
        "original_content": "\n\n  1. Introduction\nBlockchain has recently been used in various fields\u00a0[\n1\n,\n2\n,\n3\n]. The\u00a0reason why blockchain can be used in such a variety of fields is because of smart contracts. A\u00a0smart contract is a digital contract based on blockchain. Smart contracts allow parties to conclude contracts without the involvement of a third party. However, as\u00a0a result of analyzing some smart contracts deployed on Ethereum, it was confirmed that greedy contracts are included\u00a0[\n4\n]. A\u00a0greed contract locks the Ether indefinitely so that it cannot be withdrawn within the contract. Therefore, executing a greedy contract causes enormous damage. There can be events in which the parity multisig wallet is frozen. Due to a code flaw discovered in the multisig wallet library smart contract, approximately 510,000 Ether users were locked indefinitely. Since these problems pose a very critical threat to the blockchain network, they must be\u00a0prevented.\nTo prevent this problem, methodologies are needed to prevent malicious activity. There are two main ways to detect malicious activity: malicious node detection and malicious smart contract detection. Among the latter, various malicious smart contract detection models have been proposed\u00a0[\n5\n,\n6\n,\n7\n] to detect malicious smart contracts. Most deep-learning-based methods detect malicious smart contracts by learning the features of smart contracts. In\u00a0addition, research has been conducted to image smart contracts and train them to implement a detection model\u00a0[\n8\n].\nHowever, since the transaction speed is directly related to the scalability of the blockchain, the transaction speed in the blockchain must also be considered. If\u00a0detection takes too much time, it incurs computational and memory overhead. However, existing deep-learning-based detection models are designed only to increase accuracy. Therefore, existing deep learning detection models may cause computational and memory overhead. Moreover, a\u00a0lot of research on IoT blockchain has been conducted recently\u00a0[\n9\n,\n10\n,\n11\n]. Therefore, a\u00a0lightweight detection model as well as a detection rate is essential. In\u00a0other words, a\u00a0lightweight deep learning detection model that does not reduce the scalability of blockchain is\u00a0needed.\nIn this paper, through XAI, we identify important instructions when deep learning detects greedy contracts. We propose a \nGreedeeptector\n trained through important instructions. \nGreedeeptector\n is a lightweight detection model for the Internet of Things (IoT). \nGreedeeptector\n identifies greedy contracts in a computationally and memory-efficient manner without compromising the scalability of the blockchain or detection accuracy. The\u00a0efficiency of speed and memory is very important for low-end devices, such as IoT devices. Also, in\u00a0terms of blockchain scalability, the\u00a0lower the computational and memory usage, the\u00a0higher the\u00a0scalability.\n  1.1. Contribution\n  1.1.1. In-Depth Analysis of Greedy Contract Instruction Using Explainable Artificial\u00a0Intelligence\nThrough integrated gradient and gradient SHAP, instructions that affect the prediction of the model are identified. We select several important instructions for benign and greedy smart contracts, and\u00a0we analyze them in depth. In\u00a0addition, we analyze which instructions have important characteristics and are frequently used in greedy contracts.\n  1.1.2. Implementation of a Lightweight Neural Network Based on Important\u00a0Instructions\nImportant instructions extracted through XAI have fewer data dimensions. We implement a lightweight model using these important instructions for the training process. The\u00a0lightweight model is about 50% lighter than the previous model. In\u00a0addition, the\u00a0lightweight model shows little loss of accuracy.\n  1.1.3. Improving the Stability of Blockchain Networks through Detection When Executing\u00a0Contracts\nUnlike previous work, our work performs greedy contract detection when the smart contract is executed. That is, detection can be performed not only for newly deployed smart contracts but also for already deployed smart contracts. Thus, it improves the stability of the blockchain network.\nThe remainder of this paper is organized as follows: In \nSection 2\n, related technologies, such as artificial neural networks, smart contracts, and\u00a0previous work, are presented. In \nSection 3\n, the\u00a0proposed method to detect greedy contracts is introduced. In \nSection 4\n, a\u00a0comparison between the default model and the lightweight model and an in-depth analysis of the contract\u2019s instructions are described. Finally, \nSection 5\n concludes the\u00a0paper.\n  2. Related\u00a0Works\n  2.1. Artificial Neural\u00a0Networks\nAn artificial neural network\u00a0[\n12\n] refers to a computer-implemented structure of the neurons in the human brain. Deep learning performs learning by stacking multiple layers of artificial neural networks and consists of an input layer, a\u00a0hidden layer, and\u00a0an output layer. The\u00a0input layer refers to the layer that receives the data to be learned. The\u00a0hidden layer calculates the weight, and\u00a0the final result is output through the output layer. Unlike machine learning, deep learning extracts and learns features from data by itself. Due to these characteristics, deep learning is used in various fields, such as computer vision, speech recognition, natural language processing, and\u00a0signal processing. Multi-layer perceptron (MLP) and convolutional neural networks (CNN)\u00a0[\n13\n] are representative deep learning models and are mainly used for classification problems. In addition, recurrent neural networks (RNNs)\u00a0[\n14\n], which are good for training time-series data, and\u00a0generative neural networks, such as generative adversarial networks (GANs)\u00a0[\n15\n], also\u00a0exist.\n  2.2. Lightweight Deep\u00a0Learning\nLightweight deep learning is deep learning that reduces the size of the model and reduces computational complexity while maintaining the performance of models with deep and complex layers\u00a0[\n16\n]. If\u00a0a non-lightweight deep learning model is used in a low-end IoT device, memory overhead may occur in the process of loading numerous parameters. Therefore, such lightweight deep learning is essential for low-end IoT devices with limited computing resources, such as mobile environments, autonomous vehicles, and\u00a0robots. We utilize XAI to implement lightweight\u00a0models.\n  2.3. Explainable Artificial Intelligence (XAI)\nArtificial intelligence is a black-box model and\u00a0inside the model learns complex relationships and characteristics about data. Therefore, it is not clear what the reasons and grounds for decisions made by AI models are. However, in\u00a0fields where important decisions must be made, such as medicine, finance, and\u00a0law, a\u00a0clear basis for the AI output may be required. XAI\u00a0[\n17\n] is a technology created to solve these problems. XAI is a technology that provides the reasons and rationale for decisions made by deep learning models. XAI can increase the reliability of deep learning technology and has the advantage of facilitating debugging to achieve the result requiired\u00a0[\n18\n]. In\u00a0addition, by\u00a0improving understanding of the training result, it can be developed into a better model. XAI can determine which features are needed for prediction. That is, it is possible to learn a deep learning model with necessary features only, excluding unnecessary features. In\u00a0this way, the\u00a0deep learning model can be optimized and a lightweight model can be implemented. Therefore, XAI is useful for implementing efficient and lightweight models. We used Captum\u00a0[\n19\n] to apply various algorithms. Captum is a unified, open-source model interpretability library for PyTorch. There are several methods of XAI (e.g., integrated gradients and gradient SHAP). We utilize these two algorithms to implement a lightweight deep learning\u00a0model.\nIntegrated gradients (IG)\u00a0[\n20\n] is an XAI algorithm that calculates the importance of each feature by accumulating and multiplying the gradient information and the difference between the input and baseline. Since IG calculates the importance of each feature locally, it does not have a global interpretation function. IG satisfies the sensitivity and implementation invariance conditions. The\u00a0sensitivity condition states that, if the difference between the baseline and the input is only one feature, the\u00a0model should have non-zero attribution if it makes different predictions for the two inputs. The\u00a0implementation invariance condition is a condition that when different models predict the same for the same input, the\u00a0two models must have a constant attribution for the same input. IG has the advantage of being simple to implement and can be applied to various datasets, such as text and images.\nGradient SHAP\u00a0[\n21\n] is an XAI algorithm that approximates Shapley values through gradients. The Shapley value quantifies the contribution of each player participating in the game based on game theory. In\u00a0other words, the\u00a0Shapley value represents the contribution of a specific player when they cooperate with all other players. Recently, it has mainly been used to interpret deep learning models. This allows us to interpret how the features contribute to the output. The Shapley value is useful for feature analysis because it can be analyzed both locally and globally. In addition, the\u00a0Shapley value has the advantage of high accuracy by considering the interaction between inputs. However, it also has the disadvantage that the computation increases exponentially as the number of features increases. To overcome this shortcoming, Gradient SHAP calculates the Shapley values through\u00a0gradients.\n  2.4. Blockchain\nBlockchain is a peer-to-peer distributed ledger network in which all network participants share the same ledger\u00a0[\n22\n]. All transaction data in the network are included in blocks, and\u00a0the blocks are linked to each other in a chain form. Blockchain is a decentralized method in which nodes in the network each own a ledger, rather than a method in which a central server manages data. Therefore, as\u00a0the nodes in the network directly verify the transaction, the\u00a0transaction is performed without a third party and does not require a server. If\u00a0hackers want to manipulate data, they must manipulate the blockchains of the majority nodes in the network. However, since this is virtually impossible, the\u00a0integrity of the transaction is guaranteed. Due to these advantages, blockchain is used in various fields, such as digital asset transactions and medical\u00a0care.\n  2.5. Smart\u00a0Contract\nA smart contract is a digital contract based on blockchain\u00a0[\n23\n]. In\u00a0the past, contracts were made in writing, but\u00a0smart contracts are implemented through code. Smart contracts are automatically executed only when certain conditions are met. Therefore, the\u00a0decentralization pursued by the blockchain is realized by the two parties signing a contract through a smart contract without the involvement of a third-party certification authority. Smart contracts can be written in a variety of high-level programming languages, but are typically written in the Solidity language. Code written in a high-level programming language is converted to Ethereum bytecode through a compiler and is then deployed on the blockchain.The Ethereum bytecode consists of an opcode and a value. An\u00a0opcode represents an instruction to be executed by a computer, and\u00a0there are currently 140 opcodes in Ethereum. The\u00a0deployed smart contract is executed through the Ethereum virtual machine (EVM). EVM provides an execution environment for smart contracts and executes smart contracts in the form of a bytecode. In\u00a0addition, in\u00a0order to execute a smart contract, it is necessary to have a certain amount of gas, which is used as a fee for the transaction. This is the cryptocurrency paid to miners as the computational cost of smart contract\u00a0execution.\nOnce deployed, the smart contract cannot be deleted and its code cannot be modified. This is because smart contracts are deployed on a blockchain that has immutability characteristics. Information stored on the blockchain is permanently maintained and cannot be modified. In\u00a0other words, even if errors or security vulnerabilities are found in the smart contract code, they cannot be corrected, which can cause big\u00a0problems.\n  2.6. Greedy\u00a0Contract\nA greedy contract\u00a0[\n4\n] is a smart contract that can reach a state where it locks indefinitely so that the Ether cannot be withdrawn. A\u00a0greedy contract can receive the Ether. However, since there is no instruction to process the received Ether or the instruction cannot be reached, the\u00a0Ether is locked in the contract forever. Therefore, the\u00a0Ether sent to the greedy contract cannot be recovered even by the node that deployed the contract. Executing a greedy contract can cause enormous\u00a0damage.\n  2.7. Malicious Smart Contract\u00a0Detection\nACSAC \u201918 classified malicious smart contracts into three types: greedy, suicidal, and\u00a0prodigal contracts\u00a0[\n4\n]. In\u00a0addition, the\u00a0authors implemented \nMAIAN\n, a\u00a0tool that detects malicious smart contracts through symbolic analysis. The\u00a0\nMAIAN\n tool is the most representative smart contract detection tool. It detects bug-causing transactions by processing the bytecode of smart contracts. In\u00a0the symbolic analysis method, the\u00a0symbolic execution is started at the first instruction in the bytecode. Execution then proceeds sequentially until the terminating instruction (e.g., STOP, RETURN) is found. If\u00a0a function call occurs while tracing an instruction sequence, it is searched considering the branch condition. If\u00a0the terminating instruction is not valid, it is backtracked in the depth-first search process to try another path. Then, concrete validation is performed to verify the results of symbolic analysis. For\u00a0this, malicious contract candidates are executed on a fake Ethereum network created through a private fork. In\u00a0order to distinguish between the three types of malicious contracts, the\u00a0following procedures are performed: Ether check (prodigal), check if the contract can be killed (suicidal), instruction check (greedy), etc. If\u00a0the contract is determined to be malicious even after this process, \nMAIAN\n finally classifies the contract as malicious.\nThe \nMAIAN\n tool is developed in Python, and\u00a0it can detect a smart contract in less than 10 s on\u00a0average.\nIn\u00a0[\n5\n], the authors proposed a system that detects malicious contracts after first detecting malicious users. This is the latest work in the field of malicious smart contract detection. In\u00a0this system, if\u00a0a user deploys a malicious smart contract, the\u00a0user is classified as a malicious node and can no longer deploy smart contracts. LSTM, GRU, and\u00a0ANN were used as models to detect malicious smart contracts. The\u00a0AUCs of LSTM, GRU, and\u00a0ANN achieved 0.99, 0.99, and\u00a00.97, respectively. In\u00a0the scenario considered, detection is performed before deploying smart contracts, so detection is not possible for smart contracts that have already been deployed.\nIn\u00a0[\n6\n], SmartCheck, which analyzes XPath patterns by converting Solidity codes into XML-based intermediate representations, was proposed. SmartCheck detects vulnerabilities in smart contracts by analyzing the XPath. However, a\u00a0weak smart contract without an XPath pattern has a critical problem in that it is difficult to\u00a0detect.\nIn\u00a0[\n7\n], SNC \u201921 proposed a model that analyzes the vulnerability of smart contracts based on machine learning by introducing a shared child node. The\u00a0model can predict eight types of vulnerabilities including re-entrancy, arithmetic, access control, unchecked low-level calls, bad randomness, front running, and\u00a0denial of service. However, the\u00a0model is not stable because it does not secure enough malicious smart\u00a0contracts.\n  3. Greedeeptector\nFor the well-known \nMAIAN\n tool, various detection methods have been proposed. In previous work\u00a0[\n5\n], the\u00a0latest deep learning-based detection technique is described, which can also detect malicious contracts, but\u00a0the number of datasets used is very small. Therefore, the\u00a0reliability is not high, and\u00a0it is difficult to state that it is robust. Also, the methods cannot perform detection on already deployed smart contracts.\nIn this paper, we present a robust greedy contract detection system using lightweight deep learning implemented through XAI. Our approach works both for contracts to be deployed and contracts that have already been deployed. Therefore, the\u00a0proposed method can ensure the safe execution of smart contracts on the current Ethereum network. In addition, we design a lightweight detection system that can operate efficiently on the blockchain. Moreover, in\u00a0this work, we evaluate and discuss our detection system. Our work is applied to the blockchain network. That is, the\u00a0deep learning model is run on blockchain nodes (e.g., computers or low-end devices). Evaluation of the execution of the actual blockchain network will be carried out in future work.\n and Algorithm 1 show the mechanism of \nGreedeeptector\n. nodes on the Ethereum network can obtain information about smart contracts and execute smart contracts. Before\u00a0executing a smart contract, each node runs \nGreedeeptector\n \u00a0 to detect a greedy contract. For this, pre-processing is performed. As\u00a0mentioned in line 2 of Algorithm\u00a01, the smart contract\u2019s bytecodes are converted to opcodes (\nhttps://github.com/daejunpark/evm-disassembler\n accessed on 11 September 2023). Then the frequency for each opcode is counted. Finally, the\u00a0opcode frequency of the target smart contract is input into \nGreedeeptector\n. If the result (\n\n\n\n\n\n\n\n\ni\ns\nG\nr\ne\ne\nd\ny\n) is 0, then the contract is executed because it is a benign contract. Conversely, if\u00a0it is a greedy contract, it is not executed.  represents the key points of the proposed\u00a0system.\n      \nAlgorithm 1\n Greedeeptector\u00a0mechanism\nRequire:\n\u00a0 Bytecode of smart contract (\n\n\n\nB\nS\nC\n), Extracted opcodes of smart contract (\n\n\n\n\nO\nP\ns\nc\n),\n\u00a0\u00a0Frequency of opcodes (\n\n\n\nF\nO\nP\n), Deep learning model for greedy contract detection\n\u00a0\u00a0(\nGreedeeptector\n)\n\u00a0\u00a0\n\n\n\n\n\n\n\nO\np\nc\no\nd\ne\ns\n={00:STOP, ..., \n\n\nF\nF\n:SELFDESTRUCT} \n Set up dictionary mapping opcodes to \n\u00a0\u00a0bytecodes\n\u00a0\u00a0for\n\u00a0\n\n\no\np\n in \n\n\n\nB\nS\nC\n\u00a0\ndo\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nif\n\u00a0\n\n\no\np\n in \n\n\n\n\n\n\n\nO\np\nc\no\nd\ne\ns\n\u00a0\nthen\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\nO\nP\ns\nc\n.append(\n\n\no\np\n)\n Bytecode to opcode\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nend if\n\u00a0\u00a0\nend for\n\u00a0\u00a0Initialize \n\n\n\nF\nO\nP\n to zero\n\u00a0\u00a0\nfor\n\u00a0\n\n\no\np\n in \n\n\n\n\nO\nP\ns\nc\n\u00a0\ndo\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n[\n\n\n]\nF\nO\nP\n[\no\np\n]\n\u2190\n\n\n\n[\n\n\n]\nF\nO\nP\n[\no\np\n]\n+1\n Calculate frequency of opcode\n\u00a0\u00a0end for\n\u00a0\u00a0\n\n\n\n\n\n\n\n\ni\ns\nG\nr\ne\ne\nd\ny\n\u2190\nGreedeeptector\n (\n\n\n\nF\nO\nP\n)\n Greedy contract detection\n\u00a0\u00a0\nif\n\u00a0\n\n\n\n\n\n\n\n\ni\ns\nG\nr\ne\ne\nd\ny\n == \n\n\n\n\nT\nr\nu\ne\n\u00a0\nthen\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Stop the smart contract\n\u00a0\u00a0\nelse\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Execute the smart contract\n\u00a0\u00a0\nend if\n\n\n\n\n\n\n\n\nFigure 1.\n\n      Diagram of our Greedeeptector.\n\n\n  \n\n\n\n\nTable 1.\n\n    Key points of the proposed system.\n  \n\n\n\n\n\n\n  3.1. Base\u00a0Model\nIn this section, we present the base model of \nGreedeeptector\n.  shows a diagram of the base model. First, the opcodes are extracted from the smart contract. The\u00a0extracted opcodes are converted into an array representing the frequency of each opcode. The\u00a0frequencies for each opcode are input to the neural network, and\u00a0the neural network classifies greedy and benign contracts. If\u00a0the target smart contract is classified as a greedy contract, its execution is\u00a0stopped.\n\n\n\n\n\n\n\n\nFigure 2.\n\n      The diagram of the base model.\n\n\n  \n\n\n  3.1.1. Pre-Processing\nAs explained earlier, the\u00a0frequency of smart contract opcodes is required to detect greedy contracts. Therefore, the\u00a0frequency of the opcode must be extracted from the bytecode of the smart contract through pre-processing.  shows the pre-processing process. A smart contract can be expressed as a bytecode composed of opcodes (instructions) and values used for operation. We extract the opcodes among them. Then, a\u00a0sequence of opcodes used in the smart contract is generated. Since one opcode is 1 byte, a\u00a0total of 256\u00a0opcodes (00 to \n\n\nF\nF\n) can exist. However, Ethereum only provides 140 opcodes. So, we generate an array of length 140 representing the opcode frequency. The index of the frequency array means each opcode and the value means the frequency. That is, it contains information about how many times each opcode is used in a smart contract. The generated array of the opcode frequency is used as a dataset for greedy contract detection.\n\n\n\n\n\n\n\n\nFigure 3.\n\n      Converting bytecode to the frequency of opcodes in pre-processing.\n\n\n  \n\n\n  3.1.2. Detection\nThe neural network is used to detect greedy contracts. Our proposed system is designed to perform detection without compromising blockchain scalability. Due to the large size of time-series models, like recurrent neural networks, we opted for a more lightweight deep neural network.  shows the structure of the deep learning network used in our work. First, the\u00a0opcode frequency data generated through the pre-processing is input to the deep learning network. Each element of the frequency array is assigned to one neuron of the input layer. That is, the\u00a0number of neurons in the input layer is equal to the number of opcodes in Ethereum. As a hidden layer, a\u00a0fully connected layer (linear layer) is used. Before\u00a0passing to the output layer, it goes through a dropout layer that discards random neurons to prevent overfitting. In\u00a0the output layer, a\u00a0value between 0 (benign) and 1 (greedy) is output as a predicted value for the data through the sigmoid activation function. Equation\u00a0(\n1\n) shows the formula for the sigmoid activation function. Finally, the\u00a0loss between the actual label and the predicted value is calculated through the binary cross-entropy loss function. Equation\u00a0(\n2\n) represents the binary cross-entropy loss formula. \ny\n hat means the continuous sigmoid function output value between 0 and 1, and\u00a0\ny\n means the discontinuous actual value. The neural network is updated to minimize the loss. Through this training process, a\u00a0neural network can distinguish greedy contracts from benign contracts.\n          \n\n\n\n\n\n\n\n(\n\n,\n\n\u0302\u00a0\n)\n=\n\u2212\n(\n\nlog\n(\n\n\u0302\u00a0\n)\n+\n(\n1\n\u2212\n\n)\nlog\n(\n1\n\u2212\n\n\u0302\u00a0\n)\n)\nB\nC\nE\n(\ny\n,\ny\n^\n)\n=\n\u2212\ny\nlog\n(\ny\n^\n)\n+\n(\n1\n\u2212\ny\n)\nlog\n(\n1\n\u2212\ny\n^\n)\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.\n\n      The structure of the base model.\n\n\n  \n\n\n shows the hyperparameters of the base model. The\u00a0hyperparameters of the base model are chosen through experimentation to achieve the optimal performance. To\u00a0the best of our knowledge, these hyperparameters yield the maximum performance. The\u00a0number of input layers is 140 (the number of opcodes used). Also, to\u00a0prevent overfitting, a\u00a0dropout value of 40% is used, and\u00a0the epoch is set to\u00a0200.\n\n\nTable 2.\n\n    Hyperparameters of the base\u00a0model.\n  \n\n\n\n\n\n\n  3.2. Lightweight Model Using\u00a0XAI\nWe used XAI to design a computation and memory-efficient lightweight model.  shows the process of implementing a lightweight model using XAI. We implemented the base model using the frequency of 140 opcodes. Subsequently, we computed the IG and SHAP values of the base model. In\u00a0, the\u00a0red color in the graph represents the IG values, while the blue represents the SHAP values. Even though there are values for all 140 opcodes, the\u00a0graph only captures 17 of them. These values indicate the influence each feature of the input data has on the prediction; the larger their absolute value, the\u00a0more significant the feature becomes. Furthermore, a\u00a0positive value suggests that the feature contributes to the greedy contract, whereas a negative value implies a contribution to the benign contract. Thus, we can identify opcodes that have a significant impact on the classification between benign and greedy\u00a0contracts.\n\n\n\n\n\n\n\n\nFigure 5.\n\n      Design of lightweight model; the red color in the graph represents the IG values, while the blue represents the SHAP values.\n\n\n  \n\n\nThe lightweight model uses only important opcodes among 140 opcodes. That is, fewer than 140 opcodes can be used. This usually allows the construction of a simpler model, since the number of input data features is reduced. As\u00a0a result, the\u00a0number of neurons in the input layer is reduced and the parameters of the neural network are reduced. Here, only opcodes that can reduce the parameters of the model as much as possible without degrading the accuracy should be selected. So, we chose the top \nn\n opcodes on the basis of experiment.\nHowever, since a large number of data samples are used for training, some of them may have outliers. If\u00a0there are outliers, the\u00a0IG and SHAP values of a specific sample have a large effect on the average. Furthermore, the\u00a0more positive the IG and SHAP values, the\u00a0higher the contribution to greedy contracts, whereas the more negative the values indicates a higher contribution to benign contracts. The reliability is then reduced, so outliers should be removed. We use the inter-quantile range (IQR)\u00a0[\n24\n] to remove outliers. In\u00a0the inter-quartile range (IQR) approach, the\u00a0dataset is segmented into quartiles, with\u00a0the 25th percentile denoted as \n\n1\nQ\n1\n and the 75th percentile as \n\n3\nQ\n3\n. The\u00a0lower bound is established at \n\n1\n\u2212\n1.5\n\u00b7\n\n\n\nQ\n1\n\u2212\n1.5\n\u00b7\nI\nQ\nR\n and the upper bound at \n\n3\n+\n1.5\n\u00b7\n\n\n\nQ\n3\n+\n1.5\n\u00b7\nI\nQ\nR\n. Any data points falling outside this interval are deemed outliers. The\u00a0constant multiplied by \n\n\n\nI\nQ\nR\n is usually 1.5. In\u00a0summary, after\u00a0removing outliers, our lightweight model can reduce the number of parameters while maintaining accuracy by learning only the top \nn\n opcodes from the average IG and\u00a0SHAP.\n shows the hyperparameters of the lightweight model designed via XAI. The\u00a0hyperparameters of the lightweight model are chosen through experimentation to achieve optimal performance. To\u00a0the best of our knowledge, these hyperparameters yield the maximum performance. In\u00a0the lightweight model, the\u00a0same epoch, batch size, dropout rate, optimizer, and\u00a0learning rate as the base model were used. In\u00a0other words, the\u00a0difference in hyperparameters between the lightweight model and the base model is the number of neurons in the input and hidden layers. In\u00a0this work, the\u00a0frequency of one opcode is assigned to one neuron. In\u00a0the lightweight model, the\u00a0number of units in the input layer is reduced because only the frequency for important opcodes is used as data (it is set experimentally, see \nSection 4.3\n). That is, since the number of features of the input data (the number of important opcodes) is reduced, the\u00a0number of neurons in the model is reduced. As\u00a0a result, lightweight models can reduce file size and computational\u00a0complexity.\n\n\nTable 3.\n\n    Hyperparameters of the lightweight\u00a0model.\n  \n\n\n\n\n\n\n  4. Experiments and\u00a0Evaluation\nFor this experiment, we use Google Colaboratory Pro+, a\u00a0cloud-based service with Ubuntu 20.04.5 LTS and GPU (Tesla T4) 15GB RAM. Python 3.9.16 and PyTorch 2.0.0 are used as the programming\u00a0environment.\n  4.1. Dataset\nIn this experiment, the\u00a0Google BigQuery dataset\u00a0(\nhttps://cloud.google.com/blog/products/data-analytics/ethereum-bigquery-public-dataset-smart-contract-analytics?hl=en\n accessed on 11 September 2023) provided by Google is used. The\u00a0Google BigQuery dataset contains datasets for smart contracts deployed on Ethereum. A\u00a0dataset of 14,716 benign and greedy contracts is generated through MAIAN, a\u00a0malicious smart contract detection tool. The\u00a0training dataset includes both benign contracts and greedy contracts. Additionally, the\u00a0test dataset also encompasses both benign and greedy contracts. The\u00a0ratio of benign to greedy contracts is set at 1:1.\n  4.2. Result for Base\u00a0Model\n shows the performance of the base model using 140 Ethereum instructions. The\u00a0base model achieves an average test F1-score of 92.6%. The\u00a0number of parameters of the base model is 15,297.\n\n\nTable 4.\n\n    Performance of the base\u00a0model.\n  \n\n\n\n\n\n\n  4.3. Result for Lightweight\u00a0Model\nWe use XAI to design a lightweight model that is computation- and memory-efficient. The\u00a0lightweight model consists of a process of extracting important opcodes, removing outliers, and\u00a0learning the frequency of important opcodes. In\u00a0this section, we discuss the implementation and performance of the lightweight\u00a0model.\n  4.4. Comparison between Base Model and Lightweight\u00a0Model\nIn this section, we report various experiments undertaken to build a lightweight model. We also compare the model size and detection accuracy of the base and lightweight models.\n  4.4.1. Performance\n shows a comparison between the base and lightweight model. The lightweight model size is reduced by 41.5%, and\u00a0the space complexity decreases as the model size decreases. The parameter (weight of the model) is reduced by 61.8%, and\u00a0the computational complexity decreases as the model has fewer parameters. The model size is a very important advantage in lightweight deep-learning models. A small-size model is equivalent to a deep-learning model with fewer parameters. The\u00a0fewer the parameters, the\u00a0smaller the amount of computation. Therefore, a\u00a0small model size means low computational complexity. Thus, when detecting greedy contracts on the blockchain, our implementation is memory and computationally efficient. The\u00a0accuracy of the base model and the lightweight model are similar. However, since we reduced the size of the model by 41.5%, this small performance loss is\u00a0negligible.\n\n\nTable 7.\n\n    Comparison between base and lightweight\u00a0model.\n  \n\n\n\n\n\n\nIn our scenario, our implementation is deployed on each node (IoT device) on the blockchain (currently, it is a prototype.). The\u00a0efficiency of speed and memory is very important for low-end devices, such as IoT devices. In\u00a0terms of blockchain scalability, the\u00a0lower the computational and memory usage, the\u00a0higher the scalability. In\u00a0other words, our target node is a low-power node on the blockchain, so the small-size model is a significant advantage on the\u00a0blockchain.\n  4.4.2. Instruction\u00a0Analysis\n and\u00a0 show a comparison of the top eight important opcodes for benign and greedy contracts depending on the XAI algorithm. In IG and SHAP, it stands out that instructions related to branching (JUMP, JUMPDEST, JUMPI) are important. JUMP is an unconditional branch instruction, and\u00a0JUMPI is a conditional branch instruction. JUMPDEST means the branch destination address. The branch instructions related to the benign contract are JUMP and JUMPDEST. Conversely, the\u00a0branching instruction associated with greedy contracts is JUMPI. JUMPI is a conditional branch instruction, and\u00a0JUMP and JUMPDEST are instructions that branch regardless of conditions. Due to greedy contract characteristics, the\u00a0greedy contract uses conditional branch instructions to bypass the ability to process the Ether\u00a0[\n4\n]. In fact, through our experiments, it is confirmed that the JUMPI instruction is an important instruction\u2014it is thought that the JUMPI instruction greatly affects classification as a greedy contract. Conversely, in\u00a0benign contracts, conditional branch instructions are not an important\u00a0characteristic.\n\n\nTable 8.\n\n    Top 8 important opcodes in the benign\u00a0contract.\n  \n\n\n\n\n\n\n\n\nTable 9.\n\n    Top 8 important opcodes in the greedy\u00a0contract.\n  \n\n\n\n\n\n\n  4.5. Comparison with Existing\u00a0Method\nMAIAN\n detects prodigal, suicidal and greedy contracts. Its file size is very small. However, the\u00a0disadvantage is that detection takes a long\u00a0time.\nThe previous work represents the latest deep-learning-based approach. It categorizes several types of malicious smart contracts into one class. Therefore, various malicious smart contracts can be detected. However, the methods used do not work on blockchain networks and are not valid for already deployed smart contracts. In\u00a0other words, it is operable only for newly deployed smart contracts. Therefore, it is not suitable for blockchain networks where numerous malicious contracts have already been deployed. In addition, data from a total of 781 contracts (650 benign contracts and 131 malicious contracts) only were used. Therefore, it is considered that the reliability of the previous approach is not high. The elements mentioned in  are the result of our implementation of the previous model as it is.\n\n\nTable 10.\n\n    Comparison of smart contract detection\u00a0methods.\n  \n\n\n\n\n\n\nMany models have been proposed in previous work, but\u00a0we reproduce the model with the highest performance. These values are then measured by the reproduced\u00a0model.\nOur method can classify greedy contracts and benign contracts. However, among\u00a0the malicious contracts that occur when using a dataset of about 80,000, the\u00a0proportion of greedy contracts is 93.4% (in the Google BigQuery dataset). Therefore, problems caused by malicious smart contracts in the blockchain can be prevented just by detecting greedy contracts. We increased the reliability and stability of the model by using a dataset 38 times larger than in previous work. Our implementation only detects greedy contracts but\u00a0has the potential to prevent most malicious smart contracts. Our lightweight model is larger than the \nMAIAN\n, but\u00a0can detect contracts significantly faster. Compared to the previous work, the\u00a0file size and the number of parameters decreased by 28.4% and 89.2%, respectively. Since our work has far fewer parameters, it is fast and memory efficient. These points represent a great advantage for the blockchain\u00a0network.\n  5. Conclusions\nIn this work, we identify and analyze opcodes that significantly impact the model in order to implement a lightweight model. Additionally, based on this, we propose a computationally and memory-efficient lightweight detection model for the IoT. Unlike previous approaches that perform detection at the smart contract deployment phase, the proposed system performs detection at the smart contract execution phase. Therefore, the proposed system can perform the detection of already deployed smart contracts. As a result, the proposed system improves the stability and scalability of blockchain networks.\nAs a summary of our experimental results, the file size of the lightweight model is reduced by 41.5% compared to the base model. Thus, when our system is running on a blockchain, it is a memory and computationally efficient system. Finally, despite being lightweight, our system has a high detection accuracy of 92.3%.\nWe tried to detect not only greedy contracts but also various malicious smart contracts, but it is not easy to collect malicious smart contract datasets other than greedy contracts. Therefore, we implemented a model that detects only greedy contracts.\nIn future work, we plan to implement a model that can detect various malicious smart contracts as well as greedy contracts. Additionally, the lightweight model is implemented to target low-end devices. Therefore, we plan to make the model lighter by using not only XAI but also knowledge distillation, quantization, and pruning methods. Afterwards, we plan to conduct additional experiments by deploying the lightweight model on low-end devices, such as Raspberry Pi.\n\n\n\n\n\n\nAuthor Contributions\nSoftware, Y.K., W.K. and H.K.; Writing\u2014original draft, Y.K.; Writing\u2014review & editing, M.L., M.S. and H.S.; Supervision, H.S. All authors have read and agreed to the published version of the manuscript.\nFunding\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2022-0-00627, Development of Lightweight BIoT technology for Highly Constrained Devices, 100%).\nConflicts of Interest\nThe authors declare no conflict of interest.\nReferences\nLiu, Z.; Jiang, L.; Osmani, M.; Demian, P. Building information management (BIM) and blockchain (BC) for sustainable building design information management framework. \nElectronics\n \n2019\n, \n8\n, 724. [\nGoogle Scholar\n] [\nCrossRef\n]\nLiu, J.; Liu, Z.; Yang, Q.; Osmani, M.; Demian, P. A Conceptual Blockchain Enhanced Information Model of Product Service Systems Framework for Sustainable Furniture. \nBuildings\n \n2022\n, \n13\n, 85. [\nGoogle Scholar\n] [\nCrossRef\n]\nLiu, Z.; Wu, T.; Wang, F.; Osmani, M.; Demian, P. Blockchain Enhanced Construction Waste Information Management: A Conceptual Framework. \nSustainability\n \n2022\n, \n14\n, 12145. [\nGoogle Scholar\n] [\nCrossRef\n]\nNikoli\u0107, I.; Kolluri, A.; Sergey, I.; Saxena, P.; Hobor, A. Finding the greedy, prodigal, and suicidal contracts at scale. In Proceedings of the 34th Annual Computer Security Applications Conference, San Juan, PR, USA, 3\u20137 December 2018; pp. 653\u2013663. [\nGoogle Scholar\n]\nShah, H.; Shah, D.; Jadav, N.K.; Gupta, R.; Tanwar, S.; Alfarraj, O.; Tolba, A.; Raboaca, M.S.; Marina, V. Deep Learning-Based Malicious Smart Contract and Intrusion Detection System for IoT Environment. \nMathematics\n \n2023\n, \n11\n, 418. [\nGoogle Scholar\n] [\nCrossRef\n]\nTikhomirov, S.; Voskresenskaya, E.; Ivanitskiy, I.; Takhaviev, R.; Marchenko, E.; Alexandrov, Y. Smartcheck: Static analysis of ethereum smart contracts. In Proceedings of the 1st International Workshop on Emerging Trends in Software Engineering for Blockchain, Gothenburg, Sweden, 27 May 2018; pp. 9\u201316. [\nGoogle Scholar\n]\nXu, Y.; Hu, G.; You, L.; Cao, C. A novel machine learning-based analysis model for smart contract vulnerability. \nSecur. Commun. Networks\n \n2021\n, \n2021\n, 1\u201312. [\nGoogle Scholar\n] [\nCrossRef\n]\nLohith, J.J.; Anusree Manoj, K.; Guru, N.; Srinivasan, P. TP-Detect: Trigram-pixel based vulnerability detection for Ethereum smart contracts. \nMultimed. Tools Appl.\n \n2023\n, 1\u201315. [\nGoogle Scholar\n] [\nCrossRef\n]\nTyagi, A.K.; Dananjayan, S.; Agarwal, D.; Thariq Ahmed, H.F. Blockchain\u2014Internet of Things Applications: Opportunities and Challenges for Industry 4.0 and Society 5.0. \nSensors\n \n2023\n, \n23\n, 947. [\nGoogle Scholar\n] [\nCrossRef\n] [\nPubMed\n]\nTaloba, A.I.; Elhadad, A.; Rayan, A.; Abd El-Aziz, R.M.; Salem, M.; Alzahrani, A.A.; Alharithi, F.S.; Park, C. A blockchain-based hybrid platform for multimedia data processing in IoT-Healthcare. \nAlex. Eng. J.\n \n2023\n, \n65\n, 263\u2013274. [\nGoogle Scholar\n] [\nCrossRef\n]\nSharma, P.; Namasudra, S.; Crespo, R.G.; Parra-Fuente, J.; Trivedi, M.C. EHDHE: Enhancing security of healthcare documents in IoT-enabled digital healthcare ecosystems using blockchain. \nInf. Sci.\n \n2023\n, \n629\n, 703\u2013718. [\nGoogle Scholar\n] [\nCrossRef\n]\nHaykin, S. \nNeural Networks and Learning Machines, 3/E\n; Pearson Education: Chennai, Tamil Nadu, India, 2009. [\nGoogle Scholar\n]\nAlbawi, S.; Mohammed, T.A.; Al-Zawi, S. Understanding of a convolutional neural network. In Proceedings of the 2017 International Conference on Engineering and Technology (ICET), Antalya, Turkey, 21\u201323 August 2017; IEEE: Piscataway, NJ, USA, 2017; pp. 1\u20136. [\nGoogle Scholar\n]\nPetneh\u00e1zi, G. Recurrent neural networks for time series forecasting. \narXiv\n \n2019\n, arXiv:1901.00069. [\nGoogle Scholar\n]\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative adversarial networks. \nCommun. ACM\n \n2020\n, \n63\n, 139\u2013144. [\nGoogle Scholar\n] [\nCrossRef\n]\nWang, C.H.; Huang, K.Y.; Yao, Y.; Chen, J.C.; Shuai, H.H.; Cheng, W.H. Lightweight deep learning: An overview. \nIEEE Consum. Electron. Mag.\n \n2022\n, 1\u201312. [\nGoogle Scholar\n] [\nCrossRef\n]\nGunning, D.; Stefik, M.; Choi, J.; Miller, T.; Stumpf, S.; Yang, G.Z. XAI\u2014Explainable artificial intelligence. \nSci. Robot.\n \n2019\n, \n4\n, eaay7120. [\nGoogle Scholar\n] [\nCrossRef\n] [\nPubMed\n]\nAli, S.; Abuhmed, T.; El-Sappagh, S.; Muhammad, K.; Alonso-Moral, J.M.; Confalonieri, R.; Guidotti, R.; Del Ser, J.; D\u00edaz-Rodr\u00edguez, N.; Herrera, F. Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence. \nInf. Fusion\n \n2023\n, \n99\n, 101805. [\nGoogle Scholar\n]\nKokhlikyan, N.; Miglani, V.; Martin, M.; Wang, E.; Alsallakh, B.; Reynolds, J.; Melnikov, A.; Kliushkina, N.; Araya, C.; Yan, S.; et al. Captum: A unified and generic model interpretability library for pytorch. \narXiv\n \n2020\n, arXiv:2009.07896. [\nGoogle Scholar\n]\nSundararajan, M.; Taly, A.; Yan, Q. Axiomatic attribution for deep networks. In Proceedings of the International Conference on Machine Learning, PMLR, Sydney, Australia, 6\u201311 August 2017; pp. 3319\u20133328. [\nGoogle Scholar\n]\nErion, G.; Janizek, J.D.; Sturmfels, P.; Lundberg, S.M.; Lee, S.I. Learning explainable models using attribution priors. In Proceedings of the International Conference on Learning Representations ICLR 2020, Addis Ababa, Ethiopia, 30 April 2020. [\nGoogle Scholar\n]\nNakamoto, S. Bitcoin: A peer-to-peer electronic cash system. \nDecentralized Bus. Rev.\n \n2008\n, \n21260\n, 1\u20139. [\nGoogle Scholar\n]\nButerin, V. A next-generation smart contract and decentralized application platform. \nWhite Pap.\n \n2014\n, \n3\n, 1\u20132. [\nGoogle Scholar\n]\nWhaley, D.L., III. The Interquartile Range: Theory and Estimation. Ph.D. Thesis, East Tennessee State University, Johnson City, TN, USA, 2005. [\nGoogle Scholar\n]\n\n\n\n\n \nFigure 1.\n\n      Diagram of our Greedeeptector.\n\n\n\n\n\n\n \nFigure 2.\n\n      The diagram of the base model.\n\n\n\n\n\n\n \nFigure 3.\n\n      Converting bytecode to the frequency of opcodes in pre-processing.\n\n\n\n\n\n\n \nFigure 4.\n\n      The structure of the base model.\n\n\n\n\n\n\n \nFigure 5.\n\n      Design of lightweight model; the red color in the graph represents the IG values, while the blue represents the SHAP values.\n\n\n\n\n\n\n \nFigure 6.\n\n      An example of outlier removal using the IQR; Before (\nleft\n), After (\nright\n).\n\n\n\n\n\n\nTable 1.\n\n    Key points of the proposed system.\n\n\n\n\nKey Points\nDescriptions\nGreedy contracts\nGreedy contracts have potential risk (e.g., Ether can be lost.).\nBlockchain network\nBlockchain network can execute smart contract (e.g., Ethereum, Bitcoin).\nExplainable artificial intelligence\nSince it can interpret the results of training, it can be used for\u00a0lightweighting.\nLightweight deep learning\nIt increases the scalability of blockchain and can be applied to lightweight\u00a0devices.\n\n\n\n\n\n\n\n\nTable 2.\n\n    Hyperparameters of the base\u00a0model.\n\n\n\n\nHyperparameters\nDescriptions\nEpoch\n200\nBatch size\n256\nUnits of the input layer\n140\nDropout\n0.4\nOptimizer (learning rate)\nAdam (0.0001)\n\n\n\n\n\n\n\n\nTable 3.\n\n    Hyperparameters of the lightweight\u00a0model.\n\n\n\n\nHyperparameters\nDescriptions\nEpoch\n200\nBatch size\n256\nUnits of the input layer\n58\nDropout\n0.4\nOptimizer (learning rate)\nAdam (0.0001)\n\n\n\n\n\n\n\n\nTable 4.\n\n    Performance of the base\u00a0model.\n\n\n\n\nPerformance Metric\nDescriptions\nTraining F1-score\n95.0%\nValidation F1-score\n92.3%\nTest F1-score\n92.6%\nThe number of parameters\n15,297\n\n\n\n\n\n\n\n\nTable 5.\n\n    Comparison of performance according to the number of opcodes used (\nCase1\n, \nCase2\n, \nCase3\n (\n\n=\n5\n,\n7\n,\n10\nk\n=\n5\n,\n7\n,\n10\n, respectively)).\n\n\n\n\n \nCase1\nCase2\nCase3\nThe number of opcodes\n58\n51\n37\nTraining F1-score\n0.93\n0.92\n0.90\nValidation F1-score\n0.92\n0.92\n0.90\nTest F1-score\n0.92\n0.91\n0.90\nThe number of parameters\n5855\n4861\n3167\n\n\n\n\n\n\n\n\nTable 6.\n\n    Performance of the lightweight\u00a0model.\n\n\n\n\nPerformance Metric\nDescriptions\nTraining F1-score\n92.6%\nValidation F1-score\n91.6%\nTest F1-score\n92.3%\nThe number of parameters\n5855\n\n\n\n\n\n\n\n\nTable 7.\n\n    Comparison between base and lightweight\u00a0model.\n\n\n\n\n\n               Model\n              \n\n               Model Size\n              \n\n                Parameters\n              \n\n                Speed\n              \n\n               F1-Score\n              \nBase\n0.89 MB\n15,297\n0.015 ms\n92.6%\nLightweight\n0.53 MB\n5855\n0.013 ms\n92.3%\n\n\n\n\n\n\n\n\nTable 8.\n\n    Top 8 important opcodes in the benign\u00a0contract.\n\n\n\n\nAlgorithm\nSorted by Values of IG and SHAP\n\n                  1\n              \n\n                 2\n              \n\n                  3\n              \n\n                  4\n              \n\n                  5\n              \n\n                  6\n              \n\n                  7\n              \n\n                 8\n              \nIG\nJUMPDEST\nDUP1\nSUB\nJUMP\nEQ\nPUSH4\nREVERT\nSLT\nSHAP\nDUP1\nSUB\nJUMP\nJUMPDEST\nPUSH4\nEQ\nMLOAD\nLT\n\n\n\n\n\n\n\n\nTable 9.\n\n    Top 8 important opcodes in the greedy\u00a0contract.\n\n\n\n\nAlgorithm\nSorted by Values of IG and SHAP\n\n                  1\n              \n\n                 2\n              \n\n                  3\n              \n\n                  4\n              \n\n                  5\n              \n\n                  6\n              \n\n                  7\n              \n\n                 8\n              \nIG\nJUMPI\nCALLVALUE\nSWAP1\nSWAP2\nSTOP\nCALLDATASIZE\nAND\nADDRESS\nSHAP\nJUMPI\nPUSH2\nCALLVALUE\nSWAP2\nPOP\nSTOP\nSWAP1\nCALLDATASIZE\n\n\n\n\n\n\n\n\nTable 10.\n\n    Comparison of smart contract detection\u00a0methods.\n\n\n\n\nCategory\nMAIAN\n\u00a0[\n4\n]\n              \n\n               Previous Work\u00a0[\n5\n]\n              \n\n               This Work (\nLM\n)\n              \nSmart contracts\nProdigal, suicidal, greedy (3 classes)\nMalicious (2 classes)\nGreedy (2 classes)\nAlgorithm\nSymbolic analysis, Concrete validation\nDeep learning\nDeep learning\nParameters\n-\n54,018\n5855\nFile size\n0.232 MB\n0.74 MB\n0.53 MB\nSpeed\nWithin 10 s\nSlower than ours\n0.013 ms\n\n\n\n\n\n\n\n\nDisclaimer/Publisher\u2019s Note:\n The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\n\u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (\nhttps://creativecommons.org/licenses/by/4.0/\n).\n\n\n\n"
    }
}